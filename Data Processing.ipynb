{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from shutil import copyfile, move\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, models, transforms\n",
    "import moviepy.editor as moviepy\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer\n",
    "Does the following:\n",
    "\n",
    "1. Convert coloured images to grayscale\n",
    "2. Crop images according to facial features using haar cascade classifier\n",
    "3. Resize image accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        opencvImage = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        gray = cv2.cvtColor(opencvImage, cv2.COLOR_BGR2GRAY)\n",
    "        tripleGray = np.stack((gray,)*3, axis=-1)\n",
    "        faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "                tripleGray,\n",
    "                scaleFactor=1.3,\n",
    "                minNeighbors=3,\n",
    "                minSize=(30, 30)\n",
    "        )\n",
    "        for (x, y, w, h) in faces:\n",
    "            if len(faces)==1:\n",
    "                tripleGray[y:y+h, x:x+w]\n",
    "        if isinstance(self.output_size, int):\n",
    "            resized = cv2.resize(tripleGray, (self.output_size, self.output_size))\n",
    "        if isinstance(self.output_size, tuple):\n",
    "            resized = cv2.resize(tripleGray, self.output_size)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Datasets\n",
    "The following code applies the custom transformation to the specified dataset, transforming and storing the data as tensors. The initial datset should be in a folder with the following subfolders:\n",
    "- anger\n",
    "- disgust\n",
    "- fear\n",
    "- happy\n",
    "- neutral\n",
    "- sadness\n",
    "- surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CK+\n",
    "source_dir = os.getcwd()+'\\\\datasets\\\\CK+'\n",
    "target_dir = os.getcwd()+'\\\\datasets\\\\CK+ tensors\\\\'\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "count = 0\n",
    "image_dataset = datasets.ImageFolder(source_dir, data_transforms)   \n",
    "for tensor in tqdm(image_dataset):\n",
    "    torch.save(tensor, target_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FER2013\n",
    "source_dir = os.getcwd()+'\\\\datasets\\\\FER2013'\n",
    "target_dir = os.getcwd()+'\\\\datasets\\\\FER2013 tensors\\\\'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "count = 0\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(source_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'test']}\n",
    "\n",
    "# for tensor in tqdm(image_datasets['train']):\n",
    "#     torch.save(tensor, target_dir+'train\\\\'+str(count)+'.pt')\n",
    "#     count+=1\n",
    "\n",
    "count = 0\n",
    "for tensor in tqdm(image_datasets['test']):\n",
    "    torch.save(tensor, target_dir+'test\\\\'+str(count)+'.pt')\n",
    "    count+=1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AffectNet dataset\n",
    "The transformation of the dataset for AffectNet is different from the rest of the datasets. The emotional labels for the dataset is stored separately in a `automatically_annotated.csv`, which is referenced to create subfolders with image copies where the normal transformation steps can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AffectNet - sort images into respective label folders\n",
    "source_dir = os.getcwd()+'\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\Automatically_Annotated\\\\Automatically_Annotated_Images\\\\'\n",
    "target_dir = os.getcwd()+'\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\Automatically_Annotated\\\\AffectNet_sorted\\\\'\n",
    "label_dir = os.getcwd()+ '\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\Automatically_Annotated\\\\automatically_annotated.csv'\n",
    "\n",
    "df = pd.read_csv(label_dir)\n",
    "for index, row in tqdm(df.iterrows()):\n",
    "\n",
    "    imageName = row['subDirectory_filePath'].split(\"/\")\n",
    "    if row['expression']==0:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\neutral\\\\\"+imageName[1])\n",
    "    if row['expression']==1:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\happy\\\\\"+imageName[1])\n",
    "    if row['expression']==2:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\sadness\\\\\"+imageName[1])\n",
    "    if row['expression']==3:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\suprise\\\\\"+imageName[1])\n",
    "    if row['expression']==4:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\fear\\\\\"+imageName[1])\n",
    "    if row['expression']==5:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\disgust\\\\\"+imageName[1])\n",
    "    if row['expression']==6:\n",
    "        copyfile(source_dir+row['subDirectory_filePath'], target_dir+\"\\\\anger\\\\\"+imageName[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.getcwd()+'\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\Automatically_Annotated\\\\AffectNet_sorted\\\\'\n",
    "target_dir = os.getcwd()+'\\\\datasets\\\\AffectNet\\\\Automatically_Annotated_compressed\\\\Automatically_Annotated\\\\tensors\\\\'\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "count = 0\n",
    "image_dataset = datasets.ImageFolder(source_dir, data_transforms)   \n",
    "for tensor in tqdm(image_dataset):\n",
    "    if count>302887:\n",
    "        torch.save(tensor, target_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFF-Wild dataset\n",
    "\n",
    "The AFF-Wild dataset is different from the rest as it is a dataset of videos instead of images. As such we extracted out individual frames with an interval of 1 second <sup>1</sup>. The emotional label for the specific frame is referenced from the the `annotations` folder. The extracted images are saved in subfolders, where the normal transformation steps can be applied\n",
    "\n",
    "1. 30 frames constitute 1 second of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFF-Wild\n",
    "def videoToImage(fileName):\n",
    "    label_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\annotations\\\\Validation_Set\\\\\"\n",
    "    vid_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\videos\\\\Validation_Set\\\\\"\n",
    "    img_data_dir = os.getcwd()+\"\\\\datasets\\\\AFF Wild\\\\images\\\\Test_Set\\\\\"\n",
    "\n",
    "    file = open(label_data_dir+fileName, \"r\")\n",
    "    imgLabels = []\n",
    "    frameIteration = 30\n",
    "    count = 0\n",
    "    for x in file:\n",
    "        if(x[0]!='N' and x[0]!='-'):\n",
    "            if frameIteration == 30:\n",
    "                imgLabels.append({'frame': count, 'label': x[0]})\n",
    "                frameIteration = 0\n",
    "                count+=1\n",
    "            frameIteration+=1\n",
    "\n",
    "    # Get specific frames and store as images\n",
    "    images = []\n",
    "    cap = cv2.VideoCapture(vid_data_dir+fileName[:-4]+\".mp4\")\n",
    "    for x in imgLabels:\n",
    "        cap.set(1, x['frame'])\n",
    "        ret, frame = cap.read()\n",
    "        if(ret):\n",
    "            images.append(frame)\n",
    "\n",
    "    # Store images into a separate folder\n",
    "    labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "    count = 0\n",
    "    for x in images:\n",
    "        img = Image.fromarray(x, 'RGB')\n",
    "        img.save(img_data_dir+labels[int(imgLabels[count]['label'])]+'\\\\'+fileName[:-4]+'-'+str(count)+'.png')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all video files\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\AFF Wild\\\\annotations\\\\Validation_Set\\\\\"\n",
    "files = os.listdir(data_dir)\n",
    "for file in tqdm(files):\n",
    "    videoToImage(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.getcwd()+'\\\\datasets\\\\AFF Wild\\\\images\\\\'\n",
    "target_dir = os.getcwd()+'\\\\datasets\\\\AFF Wild\\\\tensors\\\\'\n",
    "\n",
    "data_transforms = {\n",
    "    'Train_Set': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "    'Test_Set': transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "count = 0\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(source_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['Train_Set', 'Test_Set']}\n",
    "\n",
    "for tensor in tqdm(image_datasets['Train_Set']):\n",
    "    torch.save(tensor, target_dir+'train\\\\'+str(count)+'.pt')\n",
    "    count+=1\n",
    "\n",
    "count = 0\n",
    "for tensor in tqdm(image_datasets['Test_Set']):\n",
    "    torch.save(tensor, target_dir+'test\\\\'+str(count)+'.pt')\n",
    "    count+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMEP dataset\n",
    "\n",
    "The GEMEP dataset is different from the rest as it is a dataset of videos instead of images. As such we extracted out individual frames with an interval of 1 second <sup>1</sup>. The emotional label for the specific frame is referenced from the the video filename. The extracted images are saved in subfolders, where the normal transformation steps can be applied\n",
    "\n",
    "1. 25 frames constitute 1 second of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GEMEP\n",
    "def videoToImage(fileName):\n",
    "    source_dir = os.getcwd()+'\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\mp4\\\\'\n",
    "    target_dir = os.getcwd()+'\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\images\\\\'\n",
    "\n",
    "    counter = 0;\n",
    "\n",
    "    # Get specific frames and store as images\n",
    "    images = []\n",
    "    frameIteration = 0\n",
    "    cap = cv2.VideoCapture(source_dir+fileName[:-4]+\".mp4\")\n",
    "    for x in range(0, int(cap.get(7))):\n",
    "        if frameIteration%25==0:\n",
    "            cap.set(1, frameIteration)\n",
    "            ret, frame = cap.read()\n",
    "            if(ret):\n",
    "                images.append(frame)\n",
    "\n",
    "    # Store images into a separate folder\n",
    "    labels = ['neutral', 'anger', 'disgust', 'fear', 'happy', 'sadness', 'suprise']\n",
    "    label = 'none'\n",
    "    if fileName[2:5] == 'ang':\n",
    "        label = 'anger'\n",
    "    elif fileName[2:5] == 'dis':\n",
    "        label = 'disgust'\n",
    "    elif fileName[2:5] == 'fea':\n",
    "        label = 'fear'\n",
    "    elif fileName[2:5] == 'sad':\n",
    "        label = 'sadness'\n",
    "    elif fileName[2:5] == 'sur':\n",
    "        label = 'suprise'\n",
    "    count = 0\n",
    "    for x in images:\n",
    "        if label == 'none':\n",
    "            break\n",
    "        img = Image.fromarray(x, 'RGB')\n",
    "        img.save(target_dir+label+'\\\\'+fileName+'-'+str(count)+'.png')\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Loop through all video files\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\avi\\\\\"\n",
    "files = os.listdir(data_dir)\n",
    "for file in files:\n",
    "    source_dir = os.getcwd()+'\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\avi\\\\'\n",
    "    target_dir = os.getcwd()+'\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\mp4\\\\'\n",
    "    clip  = moviepy.VideoFileClip(source_dir+file)\n",
    "    clip.write_videofile(target_dir+file[:-3]+\"mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all video files\n",
    "data_dir = os.getcwd() + \"\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\mp4\\\\\"\n",
    "files = os.listdir(data_dir)\n",
    "for file in tqdm(files):\n",
    "    videoToImage(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = os.getcwd() + \"\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\images\\\\\"\n",
    "target_dir = os.getcwd() + \"\\\\datasets\\\\GEMEP_Coreset Face Voice\\\\tensors\\\\\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "count = 0\n",
    "image_dataset = datasets.ImageFolder(source_dir, data_transforms)   \n",
    "for tensor in tqdm(image_dataset):\n",
    "    torch.save(tensor, target_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IASLab\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "source_dir = os.getcwd() + \"\\\\datasets\\\\IASLab\\\\images\\\\\"\n",
    "target_dir = os.getcwd() + \"\\\\datasets\\\\IASLab\\\\tensors\\\\\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        CustomTransform(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "count = 0\n",
    "image_dataset = datasets.ImageFolder(source_dir, data_transforms)   \n",
    "for tensor in tqdm(image_dataset):\n",
    "    if count>451:\n",
    "        torch.save(tensor, target_dir+str(count)+'.pt')\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
